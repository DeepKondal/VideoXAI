#!/bin/bash
#SBATCH --job-name=xai_full_pipeline        # Job name
#SBATCH --mail-type=ALL                     # Notifications for job done & fail
#SBATCH --chdir=/speed-scratch/s_abidee/XaiPortV3/XAIport  # Set working directory
#SBATCH --partition=ps                      # Explicitly specify partition
#SBATCH --nodes=1                           # Number of nodes
#SBATCH --ntasks=1                          # Number of tasks (single job)
#SBATCH --cpus-per-task=8                   # Number of CPU cores per task
#SBATCH --mem=64G                           # Memory allocation (64GB)
#SBATCH --time=24:00:00                     # 24-hour time limit
#SBATCH --output=xai_full_pipeline_%j.out   # Standard output & error log (%j = job ID)


echo " SLURM Job started at: $(date)"

#  **Load Environment Modules Properly**
if [ -f /etc/profile.d/modules.sh ]; then
    source /etc/profile.d/modules.sh
    module load python/3.8.3 || { echo " Failed to load Python module!"; exit 1; }
else
    echo " Modules system not available! Skipping..."
fi

#  **Confirm Python Version**
echo " Checking Python Version..."
python3 --version || { echo " Python3 not found! Exiting."; exit 1; }


#  **Activate Virtual Environment**
echo " Activating virtual environment..."
source /speed-scratch/s_abidee/venv/bin/activate || { echo " Failed to activate virtual environment!"; exit 1; }

#  **Ensure Pip Works Inside Virtual Environment**
#which pip3 || { echo " Pip still missing after activation! Exiting."; exit 1; }
#pip3 install --upgrade pip  # Upgrade pip inside the venv

#  **Install Dependencies**
pip3 install --no-cache-dir -r requirements_fixed.txt || { echo " Failed to install dependencies!"; exit 1; }


#  Step 6: Define function to check if a service is running
check_service() {
    local url=$1
    local retries=75
    local count=0
    echo " Checking service at $url..."

    while [[ $count -lt $retries ]]; do
        if curl -s --head --request GET "$url" | grep "200 OK" > /dev/null; then
            echo "  Service at $url is UP!"
            return 0
        fi
        echo " Waiting for service at $url ($count/$retries)..."
        sleep 5
        ((count++))
    done

    echo " Service at $url failed to start!"
    exit 1
}

#  Step 7: Start all services in order

#  **Start Data Processing Server**
echo " Starting Data Processing Server..."
nohup python3 dataprocess/dataprocess_server.py > logs/dataprocess.log 2>&1 &
sleep 5
check_service "http://127.0.0.1:8001"

#  **Start Model Server**
echo " Starting Model Server..."
nohup python3 modelserver/model_server.py > logs/modelserver.log 2>&1 &
sleep 5
check_service "http://127.0.0.1:8002/health"

#  **Start New Model Server**
echo " Starting New Model Server..."
nohup python3 new_modelserver/model_server.py > logs/new_modelserver.log 2>&1 &
sleep 5
check_service "http://127.0.0.1:8005/health"

#  **Start XAI Server**
echo " Starting XAI Server..."
nohup python3 xaiserver/xai_server.py > logs/xaiserver.log 2>&1 &
sleep 5
check_service "http://127.0.0.1:8003/health"

#  **Start Center Server**
echo " Starting Center Server..."
nohup python3 center_server.py > logs/center_server.log 2>&1 &
sleep 5
check_service "http://127.0.0.1:8880/health"

echo " All services started successfully!"

#  Step 8: Run the Pipeline Request
echo " Running the pipeline request..."
curl -X POST "http://127.0.0.1:8880/run_pipeline/" \
    -H "Content-Type: application/json" \
    -d @task_sheets/task.json || { echo " Pipeline execution failed!"; exit 1; }

echo " Pipeline execution triggered successfully!"

#  Step 9: Wait for all services to complete
wait

echo " SLURM Job completed at: $(date)"
